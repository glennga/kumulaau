\chapter{Parameter Estimation}\label{ch:parameterEstimation}
In this chapter, we discuss our approach toward parameter estimation.
Specifically, we aim to address the problem of maximum likelihood.

\section{Intractable Likelihood Functions}\label{sec:intractableLikelihoodFunctions}
\begin{figure}[t]
    \centering{\input{include/floats/likelihood-general.tex}}
    \caption{General figure depicting a smooth likelihood distribution (blue).
    The maximum likelihood point of this distribution is the intersection of the black dashed line and the blue line.
    }\label{fig:likelihoodGeneral}
\end{figure}

We denote $\theta$ to be a set of parameters values, $\Theta$ to be the space all of our parameters values reside in,
$\mathcal{D}$ to be a collection of observed data, and $\mathcal{D}'$ to
be a collection of generated data.
The \emph{likelihood} of some parameter values $\theta \in \Theta$ is the joint probability that $\theta$ produces
some observed data $\mathcal{D}$.
Let $\mathcal{M} : \theta \mapsto \mathcal{D}'$ define a random function that maps a parameter values $\theta$ to
generated data $\mathcal{D}'$.
Given that $\mathcal{M}(\theta)$ is random, different $\mathcal{D}'$ may be returned for the same parameter values
$\theta$.
%In other words, $\mathcal{M}$ represents some stochastic simulator model which produces some random data.
Given \emph{discrete and countable} observations $\mathcal{D}$, the likelihood of parameter values $\theta \in \Theta$
is the probability that our generated data equals our observed data~\cite{lintusaariFundamentalsRecentDevelopments2017}.
\begin{equation}\label{eq:likelihood1}
    \mathcal{L}(\theta) = \Pr(\mathcal{D} = \mathcal{M}(\theta))
\end{equation}

One technique to calculate a maximum likelihood for our parameter values involves finding the critical points of
$\mathcal{L}$ (where $\frac{d\mathcal{L}}{d\theta} = 0$) and choosing the point $\hat{\theta}$ with the largest value
of $\mathcal{L}$.
In~\autoref{fig:likelihoodGeneral}, the maximum of such a likelihood is displayed at the intersection of the blue and
black dashed lines.
There are however, several problems with this approach:
\begin{enumerate}
    \item A large range $\mathcal{D}'$ for simulator $\mathcal{M}$ indicates that the frequency of exact observed --
        generated matches will be too low to make inferences.
    \item This assumes that we can explicitly express $\mathcal{L}$.
        For simulator based $\mathcal{M}$ like ours, this is not trivial to do.
\end{enumerate}
Consequently, we must look into other approaches to \textit{infer} $\mathcal{L}$.
The two problems this chapter aims to address are (a) how to efficiently compute $\mathcal{L}$ for a single $\theta$
and (b) how to infer $\mathcal{L}$ for all $\theta$.

\section{Approximate Bayesian Computation}\label{sec:approximateBayesianComputation}
In this section, we discuss an approximate method to compute the likelihood $\mathcal{L}$ for some parameter values
$\theta$.

As previously mentioned, to find $\mathcal{L}$ is to find the joint probability that our model and parameters produce
the observed data.
We expand~\autoref{eq:likelihood1} to make this explicit for $r$ sets of observed data.
$\theta$ remains the same, however the randomness in $\mathcal{M}$ may result in different probabilities.
\begin{equation}
    \mathcal{L}(\theta) = \prod_{i=1}^{r} \Pr\left(\mathcal{M}(\theta) = \mathcal{D}_i \right)
\end{equation}

Let $\mathcal{D}$ represent the set of observed samples from the Columbian populace.
How do we determine the probability of our model generating some $\mathcal{D}_i \in \mathcal{D}$ to
determine $\mathcal{L}(\theta)$?
Here, we take a frequentist approach and perform the following steps:
\begin{enumerate}
    \item Run our simulator once to get $\mathcal{D}'_1$.
    \item Check if $\mathcal{D}'_1$ matches $\mathcal{D}_1$.
    \item Repeat steps 1 and 2 some arbitrary number of times $T_1$, for different simulated samples but the same
        observed sample.
        We define $\Pr\left( \mathcal{M}(\theta) = \mathcal{D}_1 \right)$ as the frequency of exact matches.
    \item Repeat step 3 for all observed samples to get $\Pr\left( \mathcal{M}(\theta) = \mathcal{D}_2 \right)$,
        \ldots, $\Pr\left( \mathcal{M}(\theta) = \mathcal{D}_r \right)$.
    \item To find the $\mathcal{L}$ is to multiply all $r$ probabilities together.
\end{enumerate}

This approach seems simple enough, but there exists one caveat: the frequency of exact matches is too low to interpret
anything meaningful.
The microsatellite repeat length set $\mathbb{M}$ consists of roughly 30 elements, meaning that roughly 30 dimensions
must exactly match.
In addition to this, each frequency resides in some large set whose elements are between $[0, 1]$.
The solution here is a technique known as \emph{Approximate Bayesian Computation}, or ABC for short.
ABC has two parts: the use of \emph{approximate} matches and the use of summary
statistics~\cite{lintusaariFundamentalsRecentDevelopments2017}.
In the following sections, I discuss how we define approximate, what summary statistics are, and why we are not using
them for this problem.

\begin{figure}[t]
    \centering{\input{include/floats/approximate-likelihood.tex}}
    \caption{General figure depicting a true likelihood surface (blue) and an approximate surface (red).
    The true likelihood may not be found in a reasonable amount time, whereas the approximate likelihood is wider but
    taller (making it more tractable, but less defined).
    }\label{fig:approximateLikelihood}
\end{figure}

\subsection{Approximate Matches: $\epsilon$}\label{subsec:approximateMatches}
Let us dissect $\mathcal{D}$ and $\mathcal{D}'$ further.
A given $\mathcal{D}_i \in \mathcal{D}$ and $\mathcal{D}'_i \in \mathcal{D}'$ define $| \mathbb{M} |$-sized tuples,
whose values exist in $[0, 1]$.
To compare a given $\mathcal{D}_i$ and $\mathcal{D}'_i$ is to iterate through each element in both tuples and measure
their proximity to each other.
Let $\delta : \mathcal{D}_i,\mathcal{D}'_i \mapsto [0, 1]$ represent some function that accepts the observed and
generated sample, and outputs some distance between 0 and 1.
The comparison between our observed and generated samples is given by $\delta(\mathcal{D}_i, \mathcal{D}'_i)$.

We were only able to explore one $\delta$ function: the angular (or Cosine) distance.
The angular distance treats $\mathcal{D}_i$ and $\mathcal{D}'_i$ as $| \mathbb{M} |$-dimensional vectors
and aims to quantify some difference between the two.
An output of 0 indicates that both samples are completely similar, while an output of 1 indicates that two vectors
are maximally dissimilar (orthogonal).
The angular distance $\delta_A$ is defined as such (normalized to make $\delta_A$ a proper distance
metric)~\cite{chaComprehensiveSurveyDistance2007a} for two samples $\mathcal{D}_i, \mathcal{D}'_i$:
\begin{equation}
    \delta_A(\mathcal{D}_i, \mathcal{D}'_i) = \frac{2}{\pi} \arccos \left(
    \frac{\sum_{\ell=\kappa}^{\Omega} \mathcal{D}_i[\ell] \cdot \mathcal{D}'_i[\ell]}{
        \sqrt{\sum_{\ell=\kappa}^{\Omega} \left(\mathcal{D}_i[\ell]\right)^2} \cdot
        \sqrt{\sum_{\ell=\kappa}^{\Omega} \left(\mathcal{D}'_i[\ell]\right)^2}
    } \right)
\end{equation}
where $\mathcal{D}_i[\ell]$ and $\mathcal{D}'_i[\ell]$ represent the frequency of repeat length $\ell$ for the observed
and generated samples respectively.

With some distance is quantified, the next step is defining what ``approximate'' means.
According to ABC, two samples $\mathcal{D}_i$ and $\mathcal{D}'_i$ are approximate matches if their distance falls below
some threshold $\epsilon \in [0, 1]$~\cite{marjoramMarkovChainMonte2003}:
\begin{equation}
    \left(\delta(\mathcal{D}_i, \mathcal{D}'_i) < \epsilon \right) \Leftrightarrow
    \left(\mathcal{D}_i \text { and } \mathcal{D}'_i
    \text{ are approximate matches } \right)
\end{equation}
By increasing $\epsilon$, the frequency of exact observed -- generated matches increases as well.
%By making this problem more tractable though, we increase the noise associated with drawing from $\mathcal{L}$.
This results in a flatter curve, as seen in~\autoref{fig:approximateLikelihood}.

The next question that follows is ``How do we know which $\epsilon$ to use?''
If $\epsilon$ is too small, the problem becomes intractable.
If $\epsilon$ is too large, we draw values that are not representative of the original distribution.
There is no clear answer to this question, and Lintusaari et.\ al.\ states this choice is typically made by
experimenting with different $(\mathcal{D}, \theta)$ pairs~\cite{lintusaariFundamentalsRecentDevelopments2017}.
We define our threshold $\epsilon$ as a \emph{hyperparameter}, a parameter we must specify and often experiment with)
to find the parameters of interest $\theta$.

\subsection{Dimension Reduction: Summary Statistics}\label{subsec:dimensionReductionSummaryStatistics}
An alternative to using a distance function $\delta$ that deals with $| \mathbb{M} |$-dimensional points is to use a
function that reduces, or \emph{summarizes} the data into two points $\bar{\mathcal{D}}, \bar{\mathcal{D}'}$ of lower
dimensionality and finds a distance between both summarized points.
We specify a distance function $\delta_S$ that performs this transformation using functions
$h : \mathcal{D}_i \mapsto \bar{\mathcal{D}_i}$ and $h' : \mathcal{D}'_i \mapsto \bar{\mathcal{D}'_i}$ as such:
\begin{equation}
    \bar{\delta_S}(\mathcal{D}_i, \mathcal{D}'_i)  = \bar{\delta}(h(\mathcal{D}_i), h'(\mathcal{D}'_i))
\end{equation}
A common choice for $h, h'$ is the mean or median.
For us, it may make sense to use the focal length computation $\hat{\ell}$ as our summary statistic.

Using summary statistics avoids the curse of dimensionality (see~\cite{bellmanDynamicProgramming2013}) for high
dimension distance functions, but this adds yet another item we must specify: ``Which summary statistic is the best?''.
If we summarize our data wrong, we again run into the problem of drawing values that do not represent our original
distribution.
We ran several trials without reducing our dimensionality and have not run into any problems thus far.
To reduce noise, we are only using the angular distance $\delta_A$ for $| \mathbb{M} |$-dimensional vectors.

\section{Markov Chain Monte Carlo}\label{sec:markovChainMonteCarlo}
In this section, we discuss the Markov Chain Monte Carlo (MCMC) approach to approximating a likelihood function.

\begin{figure}[t]
    \centering{\input{include/floats/likelihood-mcmc.tex}}
    \caption{General figure depicting the random walk of Metropolis sampler around some surface poportional to our
    likelihood (the posterior).
    We start at initial state $\theta^{(1)}$.
    We accept $\theta^{(2)}$ and $\theta^{(3)}$ which leads to regions of higher likelihood but later accept
    $\theta^{(4)}$, a less likely point due to the randomness of $\mathcal{M}$.
    }\label{fig:metropolisAlgorithm}
\end{figure}

\subsection{Monte Carlo}\label{subsec:monteCarlo}
In~\autoref{sec:approximateBayesianComputation}, we explored how to determine the likelihood of a single point $\theta$.
We are now interested in the most likely $\theta$ out of all possible parameter values $\Theta$.
We considered a naive approach to determining the most likely point $\hat{\theta}$
in~\autoref{sec:intractableLikelihoodFunctions}, which involved determining the derivative of some function we can
explicitly express.
Given that we cannot express our function as such, this is not an option.
The solution Monte Carlo algorithms propose is choosing $\theta$ randomly.

Let $p : \Theta \rightarrow [0, 1]$ define a probability distribution that determines how we draw $\theta$.
$p$ is more commonly known as a \emph{prior distribution}, and allows us to insert any prior beliefs we have about our
likelihood before finding $\mathcal{L}(\theta)$ itself.
In Bayesian inference, the characterization of the uncertainty of some $\theta$ given observations $\mathcal{D}$ is
given by another distribution known as the
\emph{posterior distribution}~\cite{lintusaariFundamentalsRecentDevelopments2017}:
\begin{equation}
    \Pr(\theta \mid \mathcal{D}) \propto \mathcal{L}(\theta) p(\theta)
\end{equation}
According to Baye's law, the posterior is proportional to likelihood function we are trying to find.
If we can explicitly express our posterior, then the $\hat{\theta}$ that maximizes $\Pr(\theta \mid \mathcal{D})$ also
maximizes $\mathcal{L}(\theta)$.

\subsection{Markov Chain Monte Carlo}\label{subsec:markovChainMonteCarlo}
With the Monte Carlo strategy, our general procedure now becomes:
\begin{enumerate}
    \item Draw $\theta_i$ from our prior $p(\theta)$.
    \item Determine the likelihood of this point $\mathcal{L}(\theta_i)$.
    \item Repeat until we have a representative set of samples.
    \item Fit our samples to a curve, and return the $\hat{\theta}$ that maximizes $\mathcal{L}(\theta)$.
\end{enumerate}
The one problem we encounter here though, is our reliance on the prior.
A misinformed prior will produce samples that are not representative of the posterior.
As an example if $E(p(\theta)) = 500$ but our posterior is centered around $\theta = 1$, we will end up with a small or
nonexistent posterior.
\emph{Markov Chain Monte Carlo} methods solve this by sampling more often from regions of higher likelihood.

Markov Chain Monte Carlo methods work by constructing a Markov chain such that the posterior distribution is its
equilibrium distribution.
We start by defining a $\Theta$-sized vector $Y$, indexed by all distinct $\theta \in \Theta$ and whose values represent
probabilities associated with each $\theta$.
$Y$ is said to be a distribution here.
Next we define a matrix $G$ of size $\Theta \times \Theta$, whose entries describe the probability of transitioning
from one $\theta$ (row) to another $\theta$ (column).
$G$ is known as a \emph{transition matrix}.
We are able to move from distribution $Y^{(i)}$ to $Y^{(i + 1)}$ using $G$:
\begin{equation}
    Y^{(i + 1)} = Y^{(i)} G
\end{equation}
$Y$ is said to be at \emph{equilibrium} if the following holds true for some transition matrix $G$:
\begin{equation}
    Y^{(i)} = Y^{(i)} G
\end{equation}
The goal of MCMC is to draw samples $X$ from $Y$ such that $Y$ represents $\mathcal{L}(\theta)$.
Let $X$ represent this \emph{chain} of states, or parameter values $\theta$, that satisfy the following
conditions to draw from such a distribution~\cite{hanadaMarkovChainMonte2018}:
\begin{enumerate}
    \item \emph{$X$ is a Markov chain}.
        The probability of obtaining $X^{(i)} \in X$ from $X^{(i - 1)} \in X$ does not depend on any other configuration
        $X^{(i - 2)}, X^{(i - 3)}, \ldots, X^{(1)}$ other than $X^{(i - 1)}$ itself.
    \item \emph{$X$ is irreducible}.
        This states that we are able to travel to all of $\Theta$ from any given $\theta$ in a finite number of
        transitions.
    \item \emph{$X$ is aperiodic for all configurations}.
        A state $\theta$ is aperiodic if there exists a $i$ such that for all $j \geq i$:
        \begin{equation}
            \Pr \left( X^{(j)} = \theta \mid X^{(1)} = \theta \right) > 0
        \end{equation}
    \item \emph{$X$ is positive recurrent}.
        The states that the expected number of transitions to move to back to the same state is finite.
\end{enumerate}
MCMC comprises a class of algorithms that are able to produce $X$ that satisfy these conditions.

\subsection{Metropolis Algorithm}\label{subsec:metropolisAlgorithm}
As per our last section, we want to generate some chain of states $X$ such that the given conditions are satisfied.
In this section, we describe the Metropolis algorithm-- a procedure that is able to generate a chain of states such that
these conditions are met.
There are three main steps to the Metropolis algorithm:
\begin{enumerate}
    \item \emph{Proposal}: We define some function $g : \Theta \rightarrow \Theta$ which generates a new $\theta_i$
        given an old $\theta'$.
        This relates to the transition matrix $G$ from~\autoref{subsec:markovChainMonteCarlo}, and constructs different
        $G$ for different values of $\theta_i$.
        The Metropolis algorithm is a special case of the \emph{Metropolis-Hastings algorithm} in which this proposal is
        symmetric.
    \item \emph{Calculate}: We determine the acceptance ratio $\alpha$, which is a ratio of the proposed $\theta^{(i)}$
        to the old $\theta'$.
    \begin{equation}
        \alpha = \frac{\mathcal{L}(\theta_i)}{\mathcal{L}(\theta)}
    \end{equation}
    \item \emph{Accept}: We save $\theta_i$ and $\mathcal{L}(\theta_i)$ to our collection of states if and only if
        $\theta_i$ is more likely than the old $\theta'$ \emph{or} the ratio of proposed to old likelihoods is greater
        than some uniform random variable $U(0, 1)$.
        If this is not true, then we go back to step (1) until we have run $T_2$ iterations.
\end{enumerate}

\begin{algorithm}[t]
    \SetAlgoLined
    \DontPrintSemicolon
    \Fn{MetropolisSampler \ {$(T_2, \mathcal{L}, \theta^{(1)}, g)$}} {
        \KwIn{number of algorithm iterations $T_2$, likelihood function $\mathcal{L}$, initial state $\theta_1$,
        proposal function $g$}
        \KwOut{samples from our posterior distribution $\Pr(\theta \mid \mathcal{D})$}
        $X \gets \emptyset$, $\theta' \gets \theta^{(1)}$ \;
        \For{$i \gets 1$ \KwTo $T_2$}{
            $\theta^{(i)} \gets g(\theta')$ \;

            \If{$\mathcal{L}(\theta^{(i)}) \cdot \mathcal{L}(\theta')^{-1} \leq \ \sim U(0, 1)$}{
                $\theta' \gets \theta_i$, $X \gets X \cup \{ \theta^{(i)} \}$ \;
            }
        }

        \Return $X$ \;
    }
%    \textbf{end} \;
    \caption{The Metropolis algorithm, used to produce samples from a posterior distribution proportional to our
    likelihood.}
    \label{alg:metropolis}
\end{algorithm}

Note that we have introduced three new hyperparameters here: our initial Markov chain position $\theta^{(1)}$,
our proposal function $g$, and the number of Metropolis sampler iterations $T_2$.
If $\theta^{(1)}$ is nowhere near the regions of high likelihood, if $g$ is too tightly or loosely distributed, or if we
do not run our sampler for enough iterations, we say that our Markov chain has not \emph{converged}.
Nonconvergence means that we cannot interpret anything meaningful from $X$, making the selection of these
hyperparameters \emph{and} some convergence verification process critical.

\section{Maximum Likelihood Estimation}\label{sec:maximumLikelihoodEstimation}
In this section, we explore a high level view of our approach to maximum likelihood estimation.
Using the Metropolis sampler, we are able to get a collection of states from a distribution proportional to our
likelihood $X$.
We now want to tie this back into our original question: ``Which parameter values $\theta \in \Theta$ are the most
likely to produce our observations $\mathcal{D}$?''.
Given states $X$ from our posterior, we determine this most likely point $\theta$ by (a) constructing histograms, (b)
fitting the histograms to the equation of some distribution, and (c) determining the mean of this distribution.

We start by constructing our histogram.
For this project, each $\theta$ represents a 2-tuple of $c$ and $d$.
We start by constructing sets $C, D$ which consists of each $c$ and $d$ element for all $\theta \in X$:
\begin{align}
    C &= \{c \mid c \in \theta \land \theta \in X \} \\
    D &= \{d \mid d \in \theta \land \theta \in X \}
\end{align}
We now want to partitioning each set $C$ and $D$ sets into consecutive, non-overlapping intervals or \emph{bins}
$C^\star$ and $D^\star$ respectively.
The partitioning itself is governed by the boundaries of our results $[\min(C), \max(C)]$,
$[\min(D),\max(D)]$, and the \emph{bin widths} $b_c$ and $b_d$ for the $c$ and $d$ parameters respectively.
This gives us the number of bins:
\begin{equation}
    \begin{aligned}
        | C^\star | &= \left\lceil \frac{\max(C) - \min(C)}{b_c} \right\rceil \\
        | D^\star | &= \left\lceil \frac{\max(D) - \min(D)}{b_d} \right\rceil
    \end{aligned}
\end{equation}
We build our histogram for $c$ by constructing the function $w_c : [0, |C^\star |] \rightarrow [0, 1]$ that accepts
an integer that enumerates our bins and outputs the frequency of associated with that bin.
The end result must follow the property in~\autoref{eq:densityFunction} to ensure that the resulting histogram
represents some probability density function.
\begin{equation} \label{eq:densityFunction}
    \int_{C^\star} w(i) di = 1
\end{equation}

The next step is to fit our histogram $w_c$ to some known distribution.
We explored two distributions here: the normal distribution and the gamma distribution.
If we assume that our posterior for some $C$ is normally distributed, we can use the function below:
\begin{equation}\label{eq:normalDistribution}
    w_{cN}(c) = \frac{\exp(\frac{\left( \sfrac{(c - \mathit{loc})}{\mathit{scale}} \right)^2}{2} )}
    {\mathit{scale} \cdot \sqrt{2\pi}}
\end{equation}
To get the point of maximum likelihood point $\hat{c}$ using $w_{N}(c)$ is to obtain its mean.
For the distribution in~\autoref{eq:normalDistribution}, this is the $\mathit{loc}$ parameter with variance
$\mathit{scale}$.
If we instead assume our posterior for some $C$ is gamma distributed, we can use the function below:
\begin{equation}
    w_{c\Gamma}(c) = \frac{(x-\mathit{loc})^{a-1}\exp(-\frac{c-\mathit{loc}}{\mathit{scale}})}{\mathit{scale}^a
    \cdot \Gamma(a)}
\end{equation}
where $\mathit{loc}$ represents a horizontal shifting parameter, $\mathit{scale}$ represents the scaling parameter of
our distribution, and $a$ represents the gamma distribution \emph{shape} or skew parameter.
Again, the maximum likelihood $\hat{c}$ using this $w_\Gamma(c)$ is the mean.
For the distribution given above, this is equal to $a \cdot \mathit{scale} + \mathit{loc}$ with variance
$a \cdot \mathit{scale}^2$.
For both cases, these steps are then repeated for $D$ to obtain $\hat{d}$.
