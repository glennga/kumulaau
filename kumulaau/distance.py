#!/usr/bin/env python3
from numpy import ndarray, zeros, mean, std, sqrt, sum, nonzero
from typing import List, Callable, Sequence
from argparse import Namespace
from numba import jit


# Location of the pool singleton.
_pool_singleton = None


@jit(nopython=True, nogil=True, target='cpu', parallel=True)
def _mean_length(sample: ndarray, kappa: int, omega: int) -> float:
    """ Given individuals from a population of microsatellites, determine the average repeat length.

    :param sample: Vector of repeat lengths to determine the average of.
    :param kappa: Repeat length lower bound.
    :param omega: Repeat length upper bound.
    :return: Mean length of sample.
    """
    return mean(sample) / float(omega - kappa)


@jit(nopython=True, nogil=True, target='cpu', parallel=True)
def _deviation_length(sample: ndarray, kappa: int, omega: int) -> float:
    """ Given individuals from a population of microsatellites, determine the deviation of repeat lengths.

    :param sample: Vector of repeat lengths to determine the deviation of.
    :param kappa: Repeat length lower bound.
    :param omega: Repeat length upper bound.
    :return: Deviation of sample.
    """
    return std(sample) / float(omega - kappa)


@jit(nopython=True, nogil=True, target='cpu', parallel=True)
def _frequency_length(sample: ndarray, ell: int) -> float:
    """ Given a sample of repeat lengths a repeat length, determine the frequency of that repeat length.

    :param sample: Vector of repeat lengths.
    :param ell: Repeat length to determine frequency of.
    :return: Frequency of ell in sample.
    """
    return len(nonzero(sample == ell)) / float(sample.size)


def summary_factory(summary: List, bounds: Sequence = None):
    """ Factory method for our summary statistics. If frequency_length is specified, then we must create a
    weight vector weighing each frequency less to reduce the impact of increasing our dimensionality.

    :param summary: Collection of summary statistics in space ['mean', 'deviation', 'frequency'].
    :param bounds: Two element sequence containing kappa, followed by omega.
    :return: Two element list of the summarizer method and the weights to apply to the resulting vector.
    """
    from numpy import concatenate, array

    # If frequency length is specify, we weight sum of frequencies as equally as other stats.
    w, kappa, omega, is_frequency_passed = 1.0 / len(summary), bounds[0], bounds[1], 'frequency' in summary
    if not is_frequency_passed:
        weights = array(list(map(lambda a: w, range(0, len(summary)))))
    else:
        weights = array(list(map(lambda a: w, range(0, len(summary) - 1))))
        weights = concatenate([weights, list(map(  # Frequency statistics come last in vector.
            lambda a: w / float(omega - kappa), range(0, omega - kappa)
        ))])

    summary_funcs = array([a for a in [  # We need reduce this to integers for Numba.
        0 if 'mean' in summary else None,
        1 if 'deviation' in summary else None,
        2 if 'frequency' in summary else None
    ] if a is not None])

    @jit(nopython=True, nogil=True, target='cpu', parallel=True)
    def _summarizer(sample: ndarray) -> ndarray:
        freq_offset, summary_vector = 0, zeros(
            len(summary_funcs) if not is_frequency_passed else len(summary_funcs) - 1 + omega - kappa
        )

        for func in summary_funcs:
            if func == 0:  # 0 maps to mean.
                summary_vector[freq_offset] = _mean_length(sample, kappa, omega)
                freq_offset += 1
            elif func == 1:  # 1 maps to deviation.
                summary_vector[freq_offset] = _deviation_length(sample, kappa, omega)
                freq_offset += 1

        if is_frequency_passed:
            for j in range(0, omega - kappa + 1):
                summary_vector[j + freq_offset] = _frequency_length(sample, j)

        return summary_vector
    return [_summarizer, weights]


@jit(nopython=True, nogil=True, target='cpu', parallel=True)
def _distance_from_summary(sample_g: ndarray, summarize: Callable, observed_s: ndarray, weights: ndarray) -> float:
    """ Obtain a weighted Euclidean distance from a summarized generated vector and observed vector.

    :param sample_g: Generated sample vector, which holds the sampled simulated population.
    :param summarize: Summary function, used to produced observed_s.
    :param observed_s: Summary vector, a resulting of putting a 'sample_g' like object through summarize.
    :param weights: Weights to associate with distance.
    :return: Distance between our generated and observed sample.
    """
    q = weights * (summarize(sample_g) - observed_s)
    return sqrt(sum(q * q.T))


def populate_d(d: ndarray, observations: Sequence, sample: Callable, summarize: Callable, weights: ndarray,
               theta_proposed) -> None:
    """ Compute the expected distance for all observations to a model generated by our proposed parameter set.

    :param d: D matrix to populate. Columns must match the length of observations. Rows indicate simulations.
    :param observations: 2D list of (int, float) tuples representing the (repeat length, frequency) tuples.
    :param sample: Function such that a population is produced with some parameter set and common ancestor.
    :param summarize: Summary function for a single distribution, generates summary vector. Must be jit compatible.
    :param weights: Weights to apply to resulting summary statistic vector.
    :param theta_proposed: The parameters associated with this matrix instance.
    :return: None.
    """
    from kumulaau.observed import tuples_to_distribution_vector
    from multiprocessing import Pool
    from numpy import array
    global _pool_singleton

    # We cannot compile this portion below, but we can parallelize it! Create a multiprocessing pool singleton.
    if _pool_singleton is None:
        _pool_singleton = Pool()

    # Generate all of our populations and save the generated data we are to compare to (bottleneck is here!!).
    sample_all = array(_pool_singleton.starmap(sample, [
        (theta_proposed, _choose_ell_0(observations, theta_proposed.kappa, theta_proposed.omega))
        for _ in range(d.shape[0])
    ]))

    # Collect the observed summary statistics into a single vector.
    observed_sv = [summarize(a) for a in tuples_to_distribution_vector(observations, sample_all[0].size)]

    # Iterate through all generated samples.
    for i in range(d.shape[0]):
        for j in range(d.shape[1]):
            d[i, j] = _distance_from_summary(sample_all[i], summarize, observed_sv[j], weights)


def _choose_ell_0(observations: Sequence, kappa: int, omega: int) -> List:
    """ We treat the starting repeat length ancestor as a nuisance parameter. We randomly choose a repeat length
    from our observed samples. If this choice exceeds our bounds, we choose our bounds instead.

    :param observations: 2D list of (int, float) tuples representing the (repeat length, frequency) tuples.
    :param kappa: Lower bound of our repeat length space.
    :param omega: Upper bound of our repeat length space.
    :return: A single repeat length, wrapped in a list.
    """
    from kumulaau.observed import tuples_to_pool
    from numpy.random import choice

    return [min(omega, max(kappa, choice(tuples_to_pool(observations))))]


def get_arguments() -> Namespace:
    """ Create the CLI and parse the arguments, if used as our main script.

    :return: Namespace of all values.
    """
    from argparse import ArgumentParser

    parser = ArgumentParser(description='Sample a simulated population and compare this to an observed data set.')
    list(map(lambda a: parser.add_argument(a[0], help=a[1], type=a[2], default=a[3], choices=a[4], nargs=a[5]), [
        ['-odb', 'Location of the observed database file.', str, 'data/observed.db', None, None],
        ['-summary', 'Summary statistics to use.', str, None, ['mean', 'deviation', 'frequency'], '*'],
        ['-uid_observed', 'ID of the observed sample to compare to.', str, None, None, None],
        ['-locus_observed', 'Locus of the observed sample to compare to.', str, None, None, None]
    ]))

    return parser.parse_args()


if __name__ == '__main__':
    from kumulaau.observed import extract_alfred_tuples
    from timeit import default_timer as timer
    from kumulaau.model import trace, evolve
    from types import SimpleNamespace
    from numpy import array

    arguments = get_arguments()  # Parse our arguments.

    # Collect observations to compare to.
    main_observed = extract_alfred_tuples([[arguments.uid_observed, arguments.locus_observed]], arguments.odb)

    # Determine our delta and sampling functions. Lambdas cannot be pickled, so we define a named function here.
    def main_sampler(theta, i_0):
        return evolve(trace(theta.n, theta.f, theta.c, theta.d, theta.kappa, theta.omega), i_0)
    main_summarizer, main_weights = summary_factory(arguments.summary, [3, 30])

    # A quick and dirty function to generate and populate the HD matrices.
    def main_generate_and_fill_d():
        d = zeros((1000, len(main_observed)), dtype='float64')
        main_theta = SimpleNamespace(n=100, f=100.0, c=0.00001, d=0.000001, kappa=3, omega=30)
        populate_d(d, main_observed, main_sampler, main_summarizer, main_weights, main_theta)
        return d

    # Run once to remove compile time in elapsed time.
    main_generate_and_fill_d()

    start_t = timer()
    main_d = main_generate_and_fill_d()  # Execute the sampling and print the running time.
    end_t = timer()
    print('Time Elapsed (1000x): [\n\t' + str(end_t - start_t) + '\n]')

    # Display our results to console, and record to our simulated database.
    print('Expected Distance: [\n\t' + str(mean(main_d)) + ' +/- ' + str(std(main_d)) + '\n]')
    print('End Matrix D: [\n\t' + str(main_d) + '\n]')
